# Decision Transformer with HCAM Memory for Offline RL

This repository contains the implementation and experimental results for equipping the Decision Transformer (DT) with Hierarchical Chunk Attention Memory (HCAM) for an MIPT internship assignment. The goal is to compare the standard DT with the DT-HCAM variant in offline reinforcement learning settings, particularly focusing on POMDPs.

## Overview

Transformers have shown success in offline RL by modeling trajectories as sequences. However, standard transformers are limited by their context window, especially in Partially Observable Markov Decision Processes (POMDPs) where long-term memory is crucial. This project investigates the use of HCAM \citep{lampinen2021towards} within the Decision Transformer \citep{chen2021decision} architecture to potentially alleviate this limitation.

I implement DT+HCAM and compare its performance against the standard DT on:
1.  **Passive T-Maze:** A simple deterministic task using an optimal dataset.
2.  **MiniGrid-MemoryS17Random-v0:** A complex, partially observable visual navigation task requiring memory, using a medium-quality dataset obtained by filtering random trajectories.

## Key Features

* Implementation of standard Decision Transformer based on \citep{chen2021decision}.
* Implementation of Hierarchical Chunk Attention Memory (HCAM) based on \citep{lampinen2021towards}.
* Integration of HCAM into DT (DT+HCAM) by replacing standard attention blocks.
* Dataset generation scripts for T-Maze (optimal) and MiniGrid-Memory (random, medium-filtered).
* Training and evaluation pipeline using PyTorch.
* Experiment configuration via YAML files.
* Logging using Weights & Biases.


## Dataset Generation

The necessary datasets can be generated using the provided scripts. The experiment shell scripts (`tmaze_experiments.sh`, `minigrid_experiments.sh`) include checks to avoid regenerating data if the files already exist.

1.  **T-Maze (Optimal):** Generated by `data_generation/generate_tmaze.py`. Run directly or via `tmaze_experiments.sh`.
    ```bash
    python -m data_generation.generate_tmaze
    ```
2.  **MiniGrid (Random & Medium):** The `minigrid_experiments.sh` script handles this. It first runs `generate_minigrid_random.py` to create the large random dataset, and then runs `filter_minigrid_data.py` to create the medium-quality filtered dataset.
    * To generate manually:
        ```bash
        # Generate random data
        python -m data_generation.generate_minigrid_random --env_id MiniGrid-MemoryS17Random-v0 --num_episodes 10000 --save_dir data/minigrid_memory

        # Filter random data
        python -m data_generation.filter_minigrid_data --input data/minigrid_memory/random_memorys17random_trajectories.npz --output data/minigrid_memory/medium_filtered_memorys17random_trajectories.npz --min_return 0.01
        ```

## Running Experiments

Use the provided shell scripts to run the training and evaluation for each environment.

**Important:** Before running, **review and adjust the corresponding `.yaml` configuration files** in the `experiments/` directory. Ensure `dataset_path`, `state_dim`, `act_dim`, `eval_target_return`, `epochs`, `learning_rate`, WandB settings, and HCAM parameters are set correctly for the experiment you want to run.

* **T-Maze Experiments:**
    ```bash
    ./experiments/tmaze_experiments.sh
    ```
    (This will run DT and DTHCAM using `config_dt_tmaze.yaml` and `config_dt_hcam_tmaze.yaml`)

* **MiniGrid Experiments:**
    ```bash
    ./experiments/minigrid_experiments.sh
    ```
    (This will first ensure data exists, then run DT and DTHCAM using `config_dt_minigrid.yaml` and `config_dt_hcam_minigrid.yaml`)

Results, including training loss and evaluation rewards, will be logged to your Weights & Biases project specified in the config files.

## Code Structure

* `envs/`: Environment definitions (custom T-Maze).
* `data_generation/`: Scripts to generate `.npz` datasets.
* `models/`: Implementations of Decision Transformer, HCAM Attention, and the combined DT+HCAM.
* `train/`: Contains the main training script (`trainer.py`), the unified evaluation logic (`evaluation.py`), and utility functions including the dataset class (`utils.py`).
* `experiments/`: YAML configuration files and shell scripts to run experiments.

## Summary of Results

* **Passive T-Maze:** Both standard DT and DT+HCAM successfully learn the optimal policy and achieve maximum reward (1.0). DTHCAM shows slightly lower training loss.
* **MiniGrid-Memory (Medium Data):** Standard DT learns a partially successful policy, achieving non-zero evaluation rewards after sufficient training (~200 epochs). DT+HCAM achieves significantly lower training loss but consistently fails during online evaluation (0.0 reward), indicating poor generalization despite better offline fitting.
* **Overall:** DTHCAM demonstrates strong offline modeling capabilities but struggles with online generalization compared to standard DT in the complex MiniGrid environment with medium-quality data, based on the current implementation and experiments.

*(Refer to the full report (`report.pdf`) and the WandB project for detailed results and analysis.)*

## References

* Chen et al. (2021). Decision transformer: Reinforcement learning via sequence modeling. *NeurIPS*.
* Lampinen et al. (2021). Towards mental time travel: a hierarchical memory for reinforcement learning agents. *NeurIPS*.
* Ni et al. (2023). When do transformers shine in rl? decoupling memory from credit assignment. *NeurIPS*.
* Chevalier-Boisvert et al. (2018-2023). Minimalistic Gridworld Environment for Gymnasium. *GitHub*.
* Vaswani et al. (2017). Attention is all you need. *NeurIPS*.

## Author

Oleg Shchendrigin
